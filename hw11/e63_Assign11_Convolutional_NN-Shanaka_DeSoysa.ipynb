{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-63 Big Data Analytics - Assignment 11 - Convolutional NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shanaka De Soysa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n",
      "sys.version_info(major=3, minor=5, micro=2, releaselevel='final', serial=0)\n",
      "TensorFlow Version: 1.0.1\n"
     ]
    }
   ],
   "source": [
    "## Printing versions for future reference\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n",
    "print(\"TensorFlow Version: {0}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Problem 1. \n",
    "Please find 2 files from Google’s tutorials sets. I used file mnist2.py in class yesterday and for preparation of my notes. If you read the file carefully you will see that you can run it in at least two modes. The way it is setup now it selects one learning rate and one particular neural network architecture and generates TensorBoard graph in a particular directory. One problem with this script is that its accuracy is surprisingly low. Such complex architecture and so many lines of code and we get 70% or lower accuracy. We expected more from Convolutional Neural Networks.  File cnn_mnist.py is practically the same, at least it does all the same things, creates the same architecture, sets the same or similar parameters, but does much better job. Its accuracy is in high 90%-s. Run two files compare results and then fix the first file (mnist2.py) based on what you saw in file cnn_mnist.py. Capture the Accuracy and Cross Entropy (summary) graphs from the corrected version of mnist2.py and provide working and fixed version of that file. Please describe in detail experiments you undertook and fixes you made. (45%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from cnn_mnist.py shows accuracies upto 99%. Our objective is to improve the mnist2.py to match this.\n",
    "![](img/p1-cnn-mnist-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Since we are benchmarking with cnn_mnist.py program, choose the parameters similar to that program. So we can compare apples to apples.\n",
    "Iterations: 500 <br>\n",
    "Learning rate: .005<br>\n",
    "use_two_fc: True<br>\n",
    "use_two_conv: True<br>\n",
    "![](img/lr005_steps500.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Bias values for both conv_layer and fc_layer have been set to constant 0.1. This could cause problems changing these values to tf.zeros for conv_layer and  truncated_normal fc_layer, that is what's been used in cnn_mnist as well.\n",
    "```python\n",
    "## For conv_layer()\n",
    "b = tf.Variable(tf.zeros([size_out], dtype=tf.float32), name=\"B\")\n",
    "\n",
    "## For fc_layer()\n",
    "b = tf.Variable(tf.truncated_normal([size_out], stddev=0.1, dtype=tf.float32), name=\"B\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### After removing constants\n",
    "![](img/lr005_steps500_remove_constants.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### That didn't really help much. Accuracy is still around 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Filter size for the conv_layer is set to a 5 x 5 matrix. This could be an issue as our images 28 x 28. Let's try changing it to 4 x4, similar to cnn_mist program.\n",
    "```python\n",
    "w = tf.Variable(tf.truncated_normal([4, 4, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](img/lr005_steps500_remove_constants_4x4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Changing the filter to 4x4 increased accuracy to about 40%.\n",
    "Let's try changing the fully connected output to **100** from 1024.\n",
    "```python\n",
    "if use_two_fc:\n",
    "    fc1 = fc_layer(flattened, 7 * 7 * conv2_features, 100, \"fc1\")\n",
    "    embedding_input = fc1\n",
    "    embedding_size = 100\n",
    "    logits = fc_layer(fc1, 100, 10, \"fc2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](img/lr005_steps500_rm_const_4x4_fc100.png)\n",
    "#### Changing the fully connected layer's output to 100 increased the accuracy to about 70%. Also played around convolution layer sizes from 32/64 to 25/50 in some cases it improved the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Let's try changing the optimizer to MomentumOptimizer.\n",
    "```python\n",
    "train_step = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(xent)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![](img/lr005_steps500_rm_const_4x4_fc100_mon.png)\n",
    "#### We can see changing the optimizer increased the accuracy about 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Let's change the fully connected layer 1 size from 100 to 512. We can clearly see that increased accuracy of 98%. Even with 100 it achieves 98% since there is randomness in the model we have to run multiple times and get average to measure it's accuracy.\n",
    "![](img/final-500-512.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Cross entropy\n",
    "![](img/final-500-512-xent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Fixed Code\n",
    "```python\n",
    "\n",
    "# Copyright 2017 Google, Inc. All Rights Reserved.\n",
    "#\n",
    "# ==============================================================================\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import urllib\n",
    "\n",
    "\n",
    "if sys.version_info[0] >= 3:\n",
    "    from urllib.request import urlretrieve\n",
    "else:\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "LOGDIR = 'log_mnist_500_512_2/'\n",
    "GITHUB_URL = 'https://raw.githubusercontent.com/mamcgrath/TensorBoard-TF-Dev-Summit-Tutorial/master/'\n",
    "GENERATIONS = 500\n",
    "\n",
    "### MNIST EMBEDDINGS ###\n",
    "mnist = tf.contrib.learn.datasets.mnist.read_data_sets(\n",
    "    train_dir=LOGDIR + 'data', one_hot=True)\n",
    "### Get a sprite and labels file for the embedding projector ###\n",
    "urlretrieve(GITHUB_URL + 'labels_1024.tsv', LOGDIR + 'labels_1024.tsv')\n",
    "urlretrieve(GITHUB_URL + 'sprite_1024.png', LOGDIR + 'sprite_1024.png')\n",
    "\n",
    "# Add convolution layer\n",
    "\n",
    "\n",
    "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        #w = tf.Variable(tf.zeros([5, 5, size_in, size_out]), name=\"W\")\n",
    "        #b = tf.Variable(tf.zeros([size_out]), name=\"B\")\n",
    "        w = tf.Variable(tf.truncated_normal(\n",
    "            [4, 4, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.zeros([size_out], dtype=tf.float32), name=\"B\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "# Add fully connected layer\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal(\n",
    "            [size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.truncated_normal(\n",
    "            [size_out], stddev=0.1, dtype=tf.float32), name=\"B\")\n",
    "        act = tf.nn.relu(tf.add(tf.matmul(input, w), b))\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act\n",
    "\n",
    "\n",
    "def mnist_model(learning_rate, use_two_conv, use_two_fc, conv1_features, conv2_features,\n",
    "                hparam, generations=500, fully_connected_size1=100):\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Setup placeholders, and reshape the data\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', x_image, 3)\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "\n",
    "    if use_two_conv:\n",
    "        conv1 = conv_layer(x_image, 1, conv1_features, \"conv1\")\n",
    "        conv_out = conv_layer(conv1, conv1_features, conv2_features, \"conv2\")\n",
    "    else:\n",
    "        conv1 = conv_layer(x_image, 1, conv2_features, \"conv\")\n",
    "        conv_out = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[\n",
    "                                  1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "    flattened = tf.reshape(conv_out, [-1, 7 * 7 * conv2_features])\n",
    "\n",
    "    if use_two_fc:\n",
    "        fc1 = fc_layer(flattened, 7 * 7 * conv2_features, 100, \"fc1\")\n",
    "        embedding_input = fc1\n",
    "        embedding_size = 100\n",
    "        logits = fc_layer(fc1, 100, 10, \"fc2\")\n",
    "    else:\n",
    "        embedding_input = flattened\n",
    "        embedding_size = 7 * 7 * conv2_features\n",
    "        logits = fc_layer(flattened, 7 * 7 * conv2_features, 10, \"fc\")\n",
    "\n",
    "    with tf.name_scope(\"xent\"):\n",
    "        xent = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y), name=\"xent\")\n",
    "        tf.summary.scalar(\"xent\", xent)\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_step = tf.train.MomentumOptimizer(\n",
    "            learning_rate, 0.9).minimize(xent)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    summ = tf.summary.merge_all()\n",
    "\n",
    "    embedding = tf.Variable(\n",
    "        tf.zeros([1024, embedding_size]), name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(LOGDIR + hparam)\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = LOGDIR + 'sprite_1024.png'\n",
    "    embedding_config.metadata_path = LOGDIR + 'labels_1024.tsv'\n",
    "    # Specify the width and height of a single thumbnail.\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(\n",
    "        writer, config)\n",
    "\n",
    "    for i in range(generations + 1):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if i % 5 == 0:\n",
    "            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={\n",
    "                                           x: batch[0], y: batch[1]})\n",
    "            writer.add_summary(s, i)\n",
    "        if i % (generations / 4) == 0:\n",
    "            sess.run(assignment, feed_dict={\n",
    "                     x: mnist.test.images[:1024], y: mnist.test.labels[:1024]})\n",
    "            saver.save(sess, os.path.join(LOGDIR, \"model.ckpt\"), i)\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})\n",
    "\n",
    "\n",
    "def make_hparam_string(learning_rate, use_two_fc, use_two_conv, conv1_features, conv2_features):\n",
    "    conv_param = \"conv2\" if use_two_conv else \"conv1\"\n",
    "    fc_param = \"fc2\" if use_two_fc else \"fc1\"\n",
    "    return \"lr_%.0E%s%s_%d_%d\" % (learning_rate, conv_param, fc_param, conv1_features, conv2_features)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # You can try adding some more learning rates\n",
    "    # for learning_rate in [1E-3, 1E-4, 1E-5]:\n",
    "    for learning_rate in [.005]:\n",
    "        # Include \"False\" as a value to try different model architectures\n",
    "        # for use_two_fc in [True, False]:\n",
    "        for use_two_fc in [True]:\n",
    "            # for use_two_conv in [True, False]:\n",
    "            for use_two_conv in [True]:\n",
    "                # for use_two_conv in [25, 32]:\n",
    "                for conv1_features in [32]:\n",
    "                    # for use_two_conv in [50, 64]:\n",
    "                    for conv2_features in [64]:\n",
    "                        # Construct a hyperparameter string for each one (example:\n",
    "                        # \"lr_1E-3fc2conv2\")\n",
    "                        hparam = make_hparam_string(\n",
    "                            learning_rate, use_two_fc, use_two_conv, conv1_features, conv2_features)\n",
    "                        print('Starting run for %s' % hparam)\n",
    "                        # this forces print-ed lines to show up.\n",
    "                        sys.stdout.flush()\n",
    "\n",
    "                        # Actually run with the new settings\n",
    "                        mnist_model(learning_rate, use_two_fc, use_two_conv, conv1_features,\n",
    "                                    conv2_features, hparam, GENERATIONS, fully_connected_size1=512)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Problem 2. \n",
    "Run corrected version of mnist2.py for 4 different architectures (2 conv, 1 conv, 2 fully connected, 1 fully connected layer) and 3 values of the learning rate. As one learning rate choose the one you selected in Problem 1 and then add one smaller and one larger learning rate around that one. Capture Accuracy (summary) graphs and One of Histograms to demonstrate to us that your code is working. Please also capture an image of “colorful” T-SNE Embedding. Please be aware that you are running 12 models and the execution might take many minutes. You might want to run your models in smaller groups so that you see them finish their work without too much wait. Submit working code of  mnist2.py used in this problem. Collect execution times, final (smoothed) accuracies and final cross entropies for different models and provide tabulated presentation of the final results of different models (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Run corrected version of mnist2.py for 4 different architectures (2 conv, 1 conv, 2 fully connected, 1 fully connected layer) and 3 values of the learning rate. As one learning rate choose the one you selected in Problem 1 and then add one smaller and one larger learning rate around that one.\n",
    "Selected learning rates: 1E-03, 5E-03, 1E-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capture Accuracy (summary) graphs and One of Histograms to demonstrate to us that your code is working. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### TensorBoard for 12 models for 2000 steps\n",
    "Convolution layer 25/50<br>\n",
    "Fully connected layer 100<br>\n",
    "![](img/p2_batch2000-cv25cv50fc100-accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross enthropy\n",
    "![](img/p2_batch2000-cv25cv50fc100-xent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect execution times, final (smoothed) accuracies and final cross entropies for different models and provide tabulated presentation of the final results of different models.\n",
    "#### Results for 2000 steps. We can see the accuracies have improved significantly close to 100% in some models.\n",
    "![](img/p2-batch2000-cv25-cv50-fc100-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms\n",
    "![](img/p2-batch2000-cv25-cv50-fc100-histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please also capture an image of “colorful” T-SNE Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/p2-batch2000-cv25-cv50-fc100-t-sne.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit working code of  mnist2.py used in this problem.\n",
    "```python\n",
    "\n",
    "# Copyright 2017 Google, Inc. All Rights Reserved.\n",
    "#\n",
    "# ==============================================================================\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import urllib\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "if sys.version_info[0] >= 3:\n",
    "    from urllib.request import urlretrieve\n",
    "else:\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "LOGDIR = 'log_mnist_fixed_25_50/'\n",
    "GITHUB_URL = 'https://raw.githubusercontent.com/mamcgrath/TensorBoard-TF-Dev-Summit-Tutorial/master/'\n",
    "GENERATIONS = 2000\n",
    "\n",
    "### MNIST EMBEDDINGS ###\n",
    "mnist = tf.contrib.learn.datasets.mnist.read_data_sets(\n",
    "    train_dir=LOGDIR + 'data', one_hot=True)\n",
    "### Get a sprite and labels file for the embedding projector ###\n",
    "urlretrieve(GITHUB_URL + 'labels_1024.tsv', LOGDIR + 'labels_1024.tsv')\n",
    "urlretrieve(GITHUB_URL + 'sprite_1024.png', LOGDIR + 'sprite_1024.png')\n",
    "\n",
    "# Add convolution layer\n",
    "\n",
    "\n",
    "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        #w = tf.Variable(tf.zeros([5, 5, size_in, size_out]), name=\"W\")\n",
    "        #b = tf.Variable(tf.zeros([size_out]), name=\"B\")\n",
    "        w = tf.Variable(tf.truncated_normal(\n",
    "            [4, 4, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        #b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        b = tf.Variable(tf.zeros([size_out], dtype=tf.float32), name=\"B\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return tf.nn.max_pool(act,\n",
    "                              ksize=[1, 2, 2, 1],\n",
    "                              strides=[1, 2, 2, 1],\n",
    "                              padding=\"SAME\")\n",
    "\n",
    "\n",
    "# Add fully connected layer\n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal(\n",
    "            [size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        #b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        #b = tf.Variable(tf.zeros([size_out], dtype=tf.float32), name=\"B\")\n",
    "        b = tf.Variable(tf.truncated_normal(\n",
    "            [size_out], stddev=0.1, dtype=tf.float32), name=\"B\")\n",
    "        act = tf.nn.relu(tf.add(tf.matmul(input, w), b))\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act\n",
    "\n",
    "\n",
    "def mnist_model(learning_rate, use_two_conv, use_two_fc,\n",
    "                hparam, conv1_features=25, conv2_features=50,\n",
    "                generations=500, fully_connected_size1=100):\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Setup placeholders, and reshape the data\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', x_image, 3)\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "\n",
    "    if use_two_conv:\n",
    "        conv1 = conv_layer(x_image, 1, conv1_features, \"conv1\")\n",
    "        conv_out = conv_layer(conv1, conv1_features, conv2_features, \"conv2\")\n",
    "    else:\n",
    "        conv1 = conv_layer(x_image, 1, conv2_features, \"conv\")\n",
    "        conv_out = tf.nn.max_pool(conv1,\n",
    "                                  ksize=[1, 2, 2, 1],\n",
    "                                  strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "    flattened = tf.reshape(conv_out, [-1, 7 * 7 * conv2_features])\n",
    "\n",
    "    if use_two_fc:\n",
    "        fc1 = fc_layer(flattened, 7 * 7 * conv2_features,\n",
    "                       fully_connected_size1, \"fc1\")\n",
    "        embedding_input = fc1\n",
    "        embedding_size = fully_connected_size1\n",
    "        logits = fc_layer(fc1, fully_connected_size1, 10, \"fc2\")\n",
    "    else:\n",
    "        embedding_input = flattened\n",
    "        embedding_size = 7 * 7 * conv2_features\n",
    "        logits = fc_layer(flattened, 7 * 7 * conv2_features, 10, \"fc\")\n",
    "\n",
    "    with tf.name_scope(\"xent\"):\n",
    "        xent = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                    labels=y), name=\"xent\")\n",
    "        tf.summary.scalar(\"xent\", xent)\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        #train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)\n",
    "        train_step = tf.train.MomentumOptimizer(\n",
    "            learning_rate, 0.9).minimize(xent)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "    summ = tf.summary.merge_all()\n",
    "\n",
    "    embedding = tf.Variable(tf.zeros([1024, embedding_size]),\n",
    "                            name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(LOGDIR + hparam)\n",
    "    writer.add_graph(sess.graph)\n",
    "\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = LOGDIR + 'sprite_1024.png'\n",
    "    embedding_config.metadata_path = LOGDIR + 'labels_1024.tsv'\n",
    "    # Specify the width and height of a single thumbnail.\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(\n",
    "        writer, config)\n",
    "\n",
    "    for i in range(generations + 1):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if i % 5 == 0:\n",
    "            [train_accuracy, s] = sess.run([accuracy, summ],\n",
    "                                           feed_dict={x: batch[0], y: batch[1]})\n",
    "            writer.add_summary(s, i)\n",
    "        if i % (generations / 4) == 0:\n",
    "            sess.run(assignment,\n",
    "                     feed_dict={x: mnist.test.images[:1024], y: mnist.test.labels[:1024]})\n",
    "            saver.save(sess, os.path.join(LOGDIR, \"model.ckpt\"), i)\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})\n",
    "    [train_accuracy, train_xent] = sess.run(\n",
    "        [accuracy, xent], feed_dict={x: batch[0], y: batch[1]})\n",
    "    return [train_accuracy, train_xent]\n",
    "\n",
    "\n",
    "def make_hparam_string(learning_rate, use_two_fc, use_two_conv):\n",
    "    conv_param = \"conv2\" if use_two_conv else \"conv1\"\n",
    "    fc_param = \"fc2\" if use_two_fc else \"fc1\"\n",
    "    return \"lr_%.0E%s%s\" % (learning_rate, conv_param, fc_param)\n",
    "\n",
    "\n",
    "def main():\n",
    "    model_metrics_cols = ['Exec. Time', 'Accuracy', 'Cross Entropy']\n",
    "    model_metrics_result = []\n",
    "    model_metrics_idx = []\n",
    "    # You can try adding some more learning rates\n",
    "    # for learning_rate in [1E-3, 1E-4, 1E-5]:\n",
    "    for learning_rate in [0.005, 1E-4, 1E-3]:\n",
    "        # Include \"False\" as a value to try different model architectures\n",
    "        # for use_two_fc in [True, False]:\n",
    "        for use_two_fc in [True, False]:\n",
    "            # for use_two_conv in [True, False]:\n",
    "            for use_two_conv in [True, False]:\n",
    "                # Construct a hyperparameter string for each one (example:\n",
    "                # \"lr_1E-3fc2conv2\")\n",
    "                hparam = make_hparam_string(learning_rate,\n",
    "                                            use_two_fc, use_two_conv)\n",
    "                print('Starting run for %s' % hparam)\n",
    "                # this forces print-ed lines to show up.\n",
    "                sys.stdout.flush()\n",
    "                start_time = time.time()\n",
    "                # Actually run with the new settings\n",
    "                accuracy, xent = mnist_model(\n",
    "                    learning_rate, use_two_fc,\n",
    "                    use_two_conv, hparam, generations=GENERATIONS,\n",
    "                    fully_connected_size1=100)\n",
    "                total_time = time.time() - start_time\n",
    "                model_metrics_idx.append(hparam)\n",
    "                model_metrics_result.append([total_time, accuracy, xent])\n",
    "                # print(model_metrics_result)\n",
    "    df = pd.DataFrame(model_metrics_result,\n",
    "                      index=model_metrics_idx,\n",
    "                      columns=model_metrics_cols)\n",
    "    print(df)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Problem 3. \n",
    "Modify file cnn_mnist.py  so that it publishes its summaries to the TensorBoard. Describe changes you are making and provide images of Accuracy and Cross Entropy summaries as captured by the Tensor Board. Provide the Graph of your model. Describe the differences if any between the graph of this program and the graph generated by mnist2.py script running with 2 convolutional and 2 fully connected layers. Provide working code.  (35%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Describe changes you are making and provide images of Accuracy and Cross Entropy summaries as captured by the Tensor Board.\n",
    "1. Added name scopes to organize models, variables, operations.<br>\n",
    "Example:\n",
    "```python\n",
    "with tf.name_scope(\"graph\"):\n",
    "    with tf.name_scope(\"variables\"):\n",
    "```\n",
    "2. Added variable names for easy identification.<br>\n",
    "Example:<br>\n",
    "```python\n",
    "x_input = tf.placeholder(\n",
    "    tf.float32, shape=x_input_shape, name=\"train_x\")\n",
    "y_target = tf.placeholder(\n",
    "    tf.int32, shape=(batch_size),  name=\"train_y\")\n",
    "conv1_weight = tf.Variable(tf.truncated_normal(\n",
    "    [4, 4, num_channels, conv1_features],\n",
    "    stddev=0.1, dtype=tf.float32), name=\"conv1_W\")\n",
    "conv1_bias = tf.Variable(tf.zeros(\n",
    "    [conv1_features],\n",
    "    dtype=tf.float32), name=\"conv1_B\")\n",
    "```\n",
    "3. Added scalar summaries for variables.<br>\n",
    "Example:<br>\n",
    "```python\n",
    "tf.summary.histogram(\"weights\", conv1_weight)\n",
    "tf.summary.histogram(\"biases\", conv1_bias)\n",
    "```\n",
    "4. Added names for operations<br>\n",
    "Example:<br>\n",
    "```python\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=model_output, labels=y_target))\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        my_optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "        train_step = my_optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        prediction = tf.nn.softmax(model_output)\n",
    "```\n",
    "\n",
    "5. Added accuracy operation<br>\n",
    "Example:<br>\n",
    "```python\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        b_pred = tf.argmax(model_output, 1)\n",
    "        # Debug\n",
    "        # b_pred = tf.Print(b_pred, [b_pred], \"b_pred = \")\n",
    "        correct_prediction = tf.equal(tf.cast(b_pred, tf.int32), y_target)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard acuracy\n",
    "![](img/p3-accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard loss\n",
    "![](img/p3-loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard Histograms\n",
    "![](img/p3-cnn-histograms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the differences if any between the graph of this program and the graph generated by mnist2.py script running with 2 convolutional and 2 fully connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Full Graph\n",
    "Main difference is that we have two graphs in this script. The training and test graphs.\n",
    "![](img/p3-graph-overall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Train and Test graphs\n",
    "![](img/p3-graph-train-test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Provide working code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Start a graph session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Load data\n",
    "data_dir = 'mnist/'\n",
    "log_dir = 'logcnn/'\n",
    "mnist = read_data_sets(data_dir)\n",
    "\n",
    "# Convert images into 28x28 (they are downloaded as 1x784)\n",
    "train_xdata = np.array([np.reshape(x, (28, 28)) for x in mnist.train.images])\n",
    "test_xdata = np.array([np.reshape(x, (28, 28)) for x in mnist.test.images])\n",
    "\n",
    "# Convert labels into one-hot encoded vectors\n",
    "train_labels = mnist.train.labels\n",
    "test_labels = mnist.test.labels\n",
    "\n",
    "# Set model parameters\n",
    "generations = 500\n",
    "\n",
    "batch_size = 100\n",
    "learning_rate = 0.005\n",
    "evaluation_size = 500\n",
    "image_width = train_xdata[0].shape[0]\n",
    "image_height = train_xdata[0].shape[1]\n",
    "target_size = max(train_labels) + 1\n",
    "num_channels = 1  # greyscale = 1 channel\n",
    "eval_every = 5\n",
    "conv1_features = 25\n",
    "conv2_features = 50\n",
    "max_pool_size1 = 2  # NxN window for 1st max pool layer\n",
    "max_pool_size2 = 2  # NxN window for 2nd max pool layer\n",
    "fully_connected_size1 = 100\n",
    "\n",
    "\n",
    "with tf.name_scope(\"graph\"):\n",
    "    with tf.name_scope(\"variables\"):\n",
    "        x_input_shape = (batch_size, image_width, image_height, num_channels)\n",
    "        x_input = tf.placeholder(\n",
    "            tf.float32, shape=x_input_shape, name=\"train_x\")\n",
    "        y_target = tf.placeholder(\n",
    "            tf.int32, shape=(batch_size),  name=\"train_y\")\n",
    "\n",
    "        eval_input_shape = (evaluation_size, image_width,\n",
    "                            image_height, num_channels)\n",
    "        eval_input = tf.placeholder(\n",
    "            tf.float32, shape=eval_input_shape,  name=\"test_x\")\n",
    "        eval_target = tf.placeholder(\n",
    "            tf.int32, shape=(evaluation_size), name=\"test_y\")\n",
    "\n",
    "        # Convolutional layer variables\n",
    "        conv1_weight = tf.Variable(tf.truncated_normal(\n",
    "            [4, 4, num_channels, conv1_features],\n",
    "            stddev=0.1, dtype=tf.float32), name=\"conv1_W\")\n",
    "        conv1_bias = tf.Variable(tf.zeros(\n",
    "            [conv1_features],\n",
    "            dtype=tf.float32), name=\"conv1_B\")\n",
    "        conv2_weight = tf.Variable(tf.truncated_normal(\n",
    "            [4, 4, conv1_features, conv2_features],\n",
    "            stddev=0.1, dtype=tf.float32), name=\"conv2_W\")\n",
    "        conv2_bias = tf.Variable(tf.zeros(\n",
    "            [conv2_features], dtype=tf.float32), name=\"conv2_B\")\n",
    "\n",
    "        # fully connected variables\n",
    "        resulting_width = image_width // (max_pool_size1 * max_pool_size2)  # 7\n",
    "        resulting_height = image_height // (max_pool_size1 * max_pool_size2) # 7\n",
    "        full1_input_size = resulting_width * resulting_height * conv2_features  # 7*7*50=2450\n",
    "\n",
    "        full1_weight = tf.Variable(tf.truncated_normal(\n",
    "            [full1_input_size, fully_connected_size1],\n",
    "            stddev=0.1, dtype=tf.float32), name=\"full1_W\")\n",
    "        full1_bias = tf.Variable(tf.truncated_normal(\n",
    "            [fully_connected_size1],\n",
    "            stddev=0.1, dtype=tf.float32), name=\"full1_B\")\n",
    "        full2_weight = tf.Variable(tf.truncated_normal(\n",
    "            [fully_connected_size1, target_size],\n",
    "            stddev=0.1, dtype=tf.float32), name=\"full2_W\")\n",
    "        full2_bias = tf.Variable(tf.truncated_normal(\n",
    "            [target_size],\n",
    "            stddev=0.1, dtype=tf.float32), name=\"full2_B\")\n",
    "\n",
    "    # Initialize Model Operations\n",
    "    def my_conv_net(input_data, graph_name):\n",
    "        # with tf.name_scope(graph_name):\n",
    "        # First Conv-ReLU-MaxPool Layer\n",
    "        with tf.name_scope(\"conv\"):\n",
    "            conv1 = tf.nn.conv2d(input_data, conv1_weight,\n",
    "                                    strides=[1, 1, 1, 1], padding='SAME')\n",
    "            relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "            max_pool1 = tf.nn.max_pool(relu1,\n",
    "                                        ksize=[1, max_pool_size1,\n",
    "                                                max_pool_size1, 1],\n",
    "                                        strides=[1, max_pool_size1,\n",
    "                                                max_pool_size1, 1],\n",
    "                                        padding='SAME')\n",
    "            tf.summary.histogram(\"weights\", conv1_weight)\n",
    "            tf.summary.histogram(\"biases\", conv1_bias)\n",
    "\n",
    "        # Second Conv-ReLU-MaxPool Layer\n",
    "        with tf.name_scope(\"conv\"):\n",
    "            conv2 = tf.nn.conv2d(max_pool1, conv2_weight,\n",
    "                                    strides=[1, 1, 1, 1], padding='SAME')\n",
    "            relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))\n",
    "            max_pool2 = tf.nn.max_pool(relu2,\n",
    "                                        ksize=[1, max_pool_size2,\n",
    "                                                max_pool_size2, 1],\n",
    "                                        strides=[1, max_pool_size2,\n",
    "                                                max_pool_size2, 1],\n",
    "                                        padding='SAME')\n",
    "            tf.summary.histogram(\"weights\", conv2_weight)\n",
    "            tf.summary.histogram(\"biases\", conv2_bias)\n",
    "\n",
    "        # Transform Output into a 1xN layer for next fully connected layer\n",
    "        with tf.name_scope(\"reshape\"):\n",
    "            final_conv_shape = max_pool2.get_shape().as_list()\n",
    "            final_shape = final_conv_shape[1] * \\\n",
    "                final_conv_shape[2] * final_conv_shape[3]\n",
    "            flat_output = tf.reshape(\n",
    "                max_pool2, [final_conv_shape[0], final_shape])\n",
    "\n",
    "        # First Fully Connected Layer\n",
    "        with tf.name_scope(\"fc\"):\n",
    "            fully_connected1 = tf.nn.relu(\n",
    "                tf.add(tf.matmul(flat_output, full1_weight), full1_bias))\n",
    "            tf.summary.histogram(\"weights\", full1_weight)\n",
    "            tf.summary.histogram(\"biases\", full1_bias)\n",
    "\n",
    "        # Second Fully Connected Layer\n",
    "        with tf.name_scope(\"fc\"):\n",
    "            final_model_output = tf.add(\n",
    "                tf.matmul(fully_connected1, full2_weight), full2_bias)\n",
    "            fully_connected1 = tf.nn.relu(\n",
    "                tf.add(tf.matmul(flat_output, full1_weight), full1_bias))\n",
    "            tf.summary.histogram(\"weights\", full2_weight)\n",
    "            tf.summary.histogram(\"biases\", full2_bias)\n",
    "\n",
    "        return final_model_output\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        model_output = my_conv_net(x_input, \"train\")\n",
    "\n",
    "        # Declare Loss Function (softmax cross entropy)\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=model_output, labels=y_target))\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            b_pred = tf.argmax(model_output, 1)\n",
    "            # Debug\n",
    "            # b_pred = tf.Print(b_pred, [b_pred], \"b_pred = \")\n",
    "            correct_prediction = tf.equal(tf.cast(b_pred, tf.int32), y_target)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "        # Create an optimizer\n",
    "        with tf.name_scope(\"optimizer\"):\n",
    "            my_optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "            train_step = my_optimizer.minimize(loss)\n",
    "\n",
    "        # Create a prediction function\n",
    "        with tf.name_scope(\"optimizer\"):\n",
    "            prediction = tf.nn.softmax(model_output)\n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"test\"):\n",
    "        test_model_output = my_conv_net(eval_input, \"test\")\n",
    "\n",
    "        # Create a prediction function\n",
    "        with tf.name_scope(\"optimizer\"):\n",
    "            test_prediction = tf.nn.softmax(test_model_output)\n",
    "\n",
    "    with tf.name_scope(\"global_ops\"):\n",
    "        # Initialize Variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        summ = tf.summary.merge_all()\n",
    "\n",
    "    # Create accuracy function\n",
    "    def get_accuracy(logits, targets):\n",
    "        batch_predictions = np.argmax(logits, axis=1)\n",
    "        num_correct = np.sum(np.equal(batch_predictions, targets))\n",
    "        ret_val = 100. * num_correct / batch_predictions.shape[0]\n",
    "        return(ret_val)\n",
    "\n",
    "sess.run(init)\n",
    "writer = tf.summary.FileWriter(log_dir)\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "\n",
    "# Start training loop\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for i in range(generations):\n",
    "    rand_index = np.random.choice(len(train_xdata),\n",
    "                                  size=batch_size)\n",
    "    rand_x = train_xdata[rand_index]\n",
    "    rand_x = np.expand_dims(rand_x, 3)\n",
    "    rand_y = train_labels[rand_index]\n",
    "    train_dict = {x_input: rand_x, y_target: rand_y}\n",
    "\n",
    "    sess.run(train_step, feed_dict=train_dict)\n",
    "    temp_train_loss, temp_train_preds, s = sess.run(\n",
    "        [loss, prediction, summ], feed_dict=train_dict)\n",
    "    temp_train_acc = get_accuracy(temp_train_preds, rand_y)\n",
    "\n",
    "    if (i + 1) % eval_every == 0:\n",
    "        # Write summaries\n",
    "        writer.add_summary(s, i)\n",
    "        eval_index = np.random.choice(len(test_xdata),\n",
    "                                      size=evaluation_size)\n",
    "        eval_x = test_xdata[eval_index]\n",
    "        eval_x = np.expand_dims(eval_x, 3)\n",
    "        eval_y = test_labels[eval_index]\n",
    "        test_dict = {eval_input: eval_x, eval_target: eval_y}\n",
    "        test_preds = sess.run(test_prediction, feed_dict=test_dict)\n",
    "        temp_test_acc = get_accuracy(test_preds, eval_y)\n",
    "\n",
    "        # Record and print results\n",
    "        train_loss.append(temp_train_loss)\n",
    "        train_acc.append(temp_train_acc)\n",
    "        test_acc.append(temp_test_acc)\n",
    "        acc_and_loss = [(i + 1), temp_train_loss,\n",
    "                        temp_train_acc, temp_test_acc]\n",
    "        acc_and_loss = [np.round(x, 2) for x in acc_and_loss]\n",
    "        print('Generation # {}. Train Loss: {:.2f}. Train Acc (Test Acc): {:.2f} ({:.2f})'.\n",
    "              format(*acc_and_loss))\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "sess.close()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
