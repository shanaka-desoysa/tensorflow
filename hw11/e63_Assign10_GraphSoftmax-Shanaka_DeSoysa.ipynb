{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# E-63 Big Data Analytics - Assignment 10 - TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shanaka De Soysa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n",
      "sys.version_info(major=3, minor=5, micro=2, releaselevel='final', serial=0)\n",
      "TensorFlow Version: 1.0.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n",
    "print(\"TensorFlow Version: {0}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.\n",
    "Please use tf_upgrade.py utility, which you could find on the TensorFlow GitHub site to upgrade attached Python script vectorized_graph.py to TensorFlow 1.x. Demonstrate that upgraded script will run and produce TensorBoard graph and summaries. Provide working upgraded script and images of your graphs and calculated summaries. (25%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.0 Upgrade Script\n",
      "-----------------------------\n",
      "Converted 1 files\n",
      "\n",
      "Detected 0 errors that require attention\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Make sure to read the detailed log 'report.txt'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python tf_upgrade.py --infile vectorized_graph.py --outfile vectorized_graph_upgraded.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect the report.txt file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Upgraded script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "LOG_FILE = 'logs/improved_graph'\n",
    "\n",
    "# Explicitly create a Graph object\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    with tf.name_scope(\"variables\"):\n",
    "        # Variable to keep track of how many times the graph has been run\n",
    "        global_step = tf.Variable(0, dtype=tf.int32, name=\"global_step\")\n",
    "\n",
    "        # Increments the above `global_step` Variable, should be run whenever\n",
    "        # the graph is run\n",
    "        increment_step = global_step.assign_add(1)\n",
    "\n",
    "        # Variable that keeps track of previous output value:\n",
    "        previous_value = tf.Variable(0.0, \n",
    "                                     dtype=tf.float32, \n",
    "                                     name=\"previous_value\")\n",
    "\n",
    "    # Primary transformation Operations\n",
    "    with tf.name_scope(\"exercise_transformation\"):\n",
    "\n",
    "        # Separate input layer\n",
    "        with tf.name_scope(\"input\"):\n",
    "            # Create input placeholder- takes in a Vector\n",
    "            a = tf.placeholder(tf.float32, \n",
    "                               shape=[None], \n",
    "                               name=\"input_placeholder_a\")\n",
    "\n",
    "        # Separate middle layer\n",
    "        with tf.name_scope(\"intermediate_layer\"):\n",
    "            b = tf.reduce_prod(a, name=\"product_b\")\n",
    "            c = tf.reduce_sum(a, name=\"sum_c\")\n",
    "\n",
    "        # Separate output layer\n",
    "        with tf.name_scope(\"output\"):\n",
    "            d = tf.add(b, c, name=\"add_d\")\n",
    "            output = tf.subtract(d, previous_value, name=\"output\")\n",
    "            update_prev = previous_value.assign(output)\n",
    "\n",
    "    # Summary Operations\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        # Creates summary for output node\n",
    "        tf.summary.scalar(\"output_summary\" ,output)\n",
    "        tf.summary.scalar(\"prod_summary\", b)\n",
    "        tf.summary.scalar(\"sum_summary\", c)\n",
    "\n",
    "    # Global Variables and Operations\n",
    "    with tf.name_scope(\"global_ops\"):\n",
    "        # Initialization Op\n",
    "        init = tf.global_variables_initializer()\n",
    "        # Collect all summary Ops in graph\n",
    "        merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "# Start a Session, using the explicitly created Graph\n",
    "sess = tf.Session(graph=graph)\n",
    "\n",
    "# Open a SummaryWriter to save summaries\n",
    "writer = tf.summary.FileWriter(LOG_FILE, graph)\n",
    "\n",
    "# Initialize Variables\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "def run_graph(input_tensor):\n",
    "    \"\"\"\n",
    "    Helper function; runs the graph with given input tensor and saves summaries\n",
    "    \"\"\"\n",
    "    feed_dict = {a: input_tensor}\n",
    "    output, summary, step = sess.run(\n",
    "        [update_prev, merged_summaries, increment_step], \n",
    "        feed_dict=feed_dict)\n",
    "    writer.add_summary(summary, global_step=step)\n",
    "\n",
    "\n",
    "# Run the graph with various inputs\n",
    "run_graph([2, 8])\n",
    "run_graph([3, 1, 3, 3])\n",
    "run_graph([8])\n",
    "run_graph([1, 2, 3])\n",
    "run_graph([11, 4])\n",
    "run_graph([4, 1])\n",
    "run_graph([7, 3, 1])\n",
    "run_graph([6, 3])\n",
    "run_graph([0, 2])\n",
    "run_graph([4, 5, 6])\n",
    "\n",
    "# Writes the summaries to disk\n",
    "writer.flush()\n",
    "\n",
    "# Flushes the summaries to disk and closes the SummaryWriter\n",
    "writer.close()\n",
    "\n",
    "# Close the session\n",
    "sess.close()\n",
    "\n",
    "# To start TensorBoard after running this file, execute the following command:\n",
    "# $ tensorboard --logdir='./improved_graph'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard Graph\n",
    "![](img\\p1_tb_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard Summaries\n",
    "![](img\\p1_tb_summaries.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2. \n",
    "Please construct a graph that will accept as inputs two vectors of equal length (tensors of dimension 1) and perform the operations on those vectors as depicted in the drawing bellow. Organize your variables and operations in nested namespaces as suggested by the nested boxes in the same graph. Organize your program in such a way that it repeats calculations in the graphs for 8 vectors of different lengths and element values. Collect and display in the TensorBoard as summaries the results on the right. (25%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](img/p2_problem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "LOG_FILE = 'logs/p2'\n",
    "\n",
    "# Explicitly create a Graph object\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    with tf.name_scope(\"variables\"):\n",
    "        # Variable to keep track of how many times the graph has been run\n",
    "        global_step = tf.Variable(0, dtype=tf.int32, name=\"global_step\")\n",
    "\n",
    "        # Increments the above `global_step` Variable, should be run whenever\n",
    "        # the graph is run\n",
    "        increment_step = global_step.assign_add(1)\n",
    "\n",
    "    a = tf.placeholder(tf.float32,\n",
    "                       shape=[None],\n",
    "                       name=\"input_a\")\n",
    "    b = tf.placeholder(tf.float32,\n",
    "                       shape=[None],\n",
    "                       name=\"input_b\")\n",
    "\n",
    "    # Primary transformation Operations\n",
    "    with tf.name_scope(\"exercise_transformation\"):\n",
    "        # Separate input layer\n",
    "        with tf.name_scope(\"intermediate_layer_1\"):\n",
    "            # Create input placeholder- takes in a Vector\n",
    "            with tf.name_scope(\"intermediate_layer_a\"):\n",
    "                a_moments = tf.nn.moments(a, axes=[0], name=\"a_sd\")\n",
    "                a_norm = tf.norm(a, name=\"a_norm\")\n",
    "\n",
    "            with tf.name_scope(\"intermediate_layer_b\"):\n",
    "                b_moments = tf.nn.moments(b, axes=[0], name=\"b_sd\")\n",
    "                b_norm = tf.norm(b, name=\"b_norm\")\n",
    "\n",
    "        # Separate middle layer\n",
    "        with tf.name_scope(\"intermediate_layer_2\"):\n",
    "            a_normalized = tf.nn.l2_normalize(a, dim=0, name=\"normalize_a\")\n",
    "            b_normalized = tf.nn.l2_normalize(b, dim=0, name=\"normalize_b\")\n",
    "\n",
    "    # Separate output layer\n",
    "    with tf.name_scope(\"cosine_ab\"):\n",
    "        b_normalized_T = tf.transpose([b_normalized])\n",
    "        cosine_similarity = tf.matmul([a_normalized],\n",
    "                                      b_normalized_T)\n",
    "\n",
    "    a_sd = tf.sqrt(a_moments[1], name=\"a_std_dev\")\n",
    "    b_sd = tf.sqrt(b_moments[1], name=\"b_std_dev\")\n",
    "\n",
    "    with tf.name_scope(\"covariance_ab\"):\n",
    "        a_mean = tf.cast(tf.reduce_mean(a), tf.float32)\n",
    "        b_mean = tf.cast(tf.reduce_mean(b), tf.float32)\n",
    "        a_delta = a - a_mean\n",
    "        b_delta = b - b_mean\n",
    "        covariance = tf.reduce_mean(tf.multiply(a_delta, b_delta))\n",
    "\n",
    "    # Summary Operations\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        # Creates summary for output node\n",
    "        tf.summary.scalar(\"a_sd\", a_sd)\n",
    "        tf.summary.scalar(\"b_sd\", b_sd)\n",
    "        tf.summary.scalar(\"a_norm\", a_norm)\n",
    "        tf.summary.scalar(\"b_norm\", b_norm)\n",
    "        tf.summary.scalar(\"cosine_ab\", cosine_similarity[0][0])\n",
    "        tf.summary.scalar(\"covariance_ab\", covariance)\n",
    "\n",
    "    # Global Variables and Operations\n",
    "    with tf.name_scope(\"global_ops\"):\n",
    "        # Initialization Op\n",
    "        init = tf.global_variables_initializer()\n",
    "        # Collect all summary Ops in graph\n",
    "        merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "# Start a Session, using the explicitly created Graph\n",
    "sess = tf.Session(graph=graph)\n",
    "\n",
    "# Open a SummaryWriter to save summaries\n",
    "writer = tf.summary.FileWriter(LOG_FILE, graph)\n",
    "\n",
    "# Initialize Variables\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "def run_graph(input_tensor1, input_tensor2):\n",
    "    \"\"\"\n",
    "    Helper function; runs the graph with given input tensor and saves summaries\n",
    "    \"\"\"\n",
    "    feed_dict = {a: input_tensor1, b: input_tensor2}\n",
    "    # a_sd_val, b_sd_val, a_norm_val, b_norm_val, cosine_similarity_val, covariance_val, summary, step = sess.run(\n",
    "    #     [a_sd, b_sd, a_norm, b_norm, cosine_similarity, covariance, merged_summaries, increment_step], feed_dict=feed_dict)\n",
    "    # print(\"a_sd: {0}, b_sd: {1}, a_norm: {2}, b_norm: {3}, cosine: {4}, covariance: {5}\".\n",
    "    # format(a_sd_val, b_sd_val, a_norm_val, b_norm_val,\n",
    "    # cosine_similarity_val, covariance_val))\n",
    "    summary, step, a_mean_val, b_mean_val, covariance_val = sess.run(\n",
    "        [merged_summaries, increment_step, a_mean, b_mean, covariance], feed_dict=feed_dict)\n",
    "    writer.add_summary(summary, step)\n",
    "    print(\"a_mean: {0}, b_mean: {1}, cov: {2}\".format(\n",
    "        a_mean_val, b_mean_val, covariance_val))\n",
    "\n",
    "\n",
    "#run_graph([3.0, 5.0, 355.0, 3.0], [22.0, 111.0, 3.0, 10.0])\n",
    "#run_graph([3, 1, 3, 3],[3, 1, 3, 3])\n",
    "\n",
    "def run_graph_with_random_vectors(iterations=8, seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    for i in range(iterations):\n",
    "        v_len = np.random.randint(10, 20)\n",
    "        print(\"Vector length: {0}\".format(v_len))\n",
    "        x, y = [], []\n",
    "        for j in range(v_len):\n",
    "            x.append(np.random.randint(1, 10))\n",
    "            y.append(np.random.randint(1, 10))\n",
    "        print(\"x: {0}\".format(x))\n",
    "        print(\"y: {0}\".format(y))\n",
    "        run_graph(x, y)\n",
    "\n",
    "\n",
    "run_graph_with_random_vectors()\n",
    "\n",
    "\n",
    "# Writes the summaries to disk\n",
    "writer.flush()\n",
    "\n",
    "# Flushes the summaries to disk and closes the SummaryWriter\n",
    "writer.close()\n",
    "\n",
    "# Close the session\n",
    "sess.close()\n",
    "\n",
    "# To start TensorBoard after running this file, execute the following command:\n",
    "# $ tensorboard --logdir='./improved_graph'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard Graph\n",
    "![](img/p2_tb_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard Summaries\n",
    "![](img/p2_tb_summaries.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3. \n",
    "Fetch Iris Dataset from https://archive.ics.uci.edu/ml/datasets/Iris and make attached Python script, softmax_irises.py work. You might have to upgrade the script to TF 1.x API. Generate TensorBoard graph of the process and use scalar summary to presenting variation of the loss function during the training process. Report the results of the evaluation process. (25%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Upgraded and improved code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pylint: disable=invalid-name\n",
    "\n",
    "# Softmax example in TF using the classical Iris dataset\n",
    "# Download iris.data from https://archive.ics.uci.edu/ml/datasets/Iris\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "DATA_FILE = \"data/IrisDataSet.csv\"\n",
    "LOG_FILE = \"logs/p3_iris\"\n",
    "\n",
    "\n",
    "def combine_inputs(X):\n",
    "    with tf.name_scope(\"combine_inputs\"):\n",
    "        return tf.matmul(X, W) + b\n",
    "\n",
    "\n",
    "def inference(X):\n",
    "    with tf.name_scope(\"inference\"):\n",
    "        return tf.nn.softmax(combine_inputs(X))\n",
    "\n",
    "\n",
    "def loss(X, Y):\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        return tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=combine_inputs(X),\n",
    "                labels=Y))\n",
    "\n",
    "\n",
    "def read_csv(batch_size, file_name, record_defaults):\n",
    "    with tf.name_scope(\"read_csv\"):\n",
    "        filename_queue = tf.train.string_input_producer(\n",
    "            [os.path.dirname(__file__) + \"/\" + file_name])\n",
    "\n",
    "        reader = tf.TextLineReader(skip_header_lines=1)\n",
    "        key, value = reader.read(filename_queue)\n",
    "\n",
    "        # decode_csv will convert a Tensor from type string (the text line) in\n",
    "        # a tuple of tensor columns with the specified defaults, which also\n",
    "        # sets the data type for each column\n",
    "        decoded = tf.decode_csv(\n",
    "            value, record_defaults=record_defaults, name=\"decode_csv\")\n",
    "\n",
    "        # batch actually reads the file and loads \"batch_size\" rows in a single\n",
    "        # tensor\n",
    "        return tf.train.shuffle_batch(decoded,\n",
    "                                      batch_size=batch_size,\n",
    "                                      capacity=batch_size * 50,\n",
    "                                      min_after_dequeue=batch_size,\n",
    "                                      name=\"shuffle_batch\")\n",
    "\n",
    "\n",
    "def inputs():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        sepal_length, sepal_width, petal_length, petal_width, label =\\\n",
    "            read_csv(100, DATA_FILE, [[0.0], [0.0], [0.0], [0.0], [\"\"]])\n",
    "\n",
    "        # convert class names to a 0 based class index.\n",
    "        label_number = tf.to_int32(tf.argmax(tf.to_int32(tf.stack([\n",
    "            tf.equal(label, [\"Iris-setosa\"]),\n",
    "            tf.equal(label, [\"Iris-versicolor\"]),\n",
    "            tf.equal(label, [\"Iris-virginica\"])\n",
    "        ])), 0), name=\"label\")\n",
    "\n",
    "        # Pack all the features that we care about in a single matrix;\n",
    "        # We then transpose to have a matrix with one example per row and one\n",
    "        # feature per column.\n",
    "        features = tf.transpose(tf.stack(\n",
    "            [sepal_length, sepal_width, petal_length, petal_width]), name=\"features\")\n",
    "\n",
    "        return features, label_number\n",
    "\n",
    "\n",
    "def train(total_loss):\n",
    "    with tf.name_scope(\"train\"):\n",
    "        learning_rate = 0.01\n",
    "        return tf.train.GradientDescentOptimizer(learning_rate, name=\"GradientDescent\").minimize(total_loss)\n",
    "\n",
    "\n",
    "def evaluate(sess, X, Y):\n",
    "    with tf.name_scope(\"evaluate\"):\n",
    "        predicted = tf.cast(tf.arg_max(inference(X), 1), tf.int32)\n",
    "        print(\"Evaluation: \", sess.run(tf.reduce_mean(\n",
    "            tf.cast(tf.equal(predicted, Y), tf.float32))))\n",
    "\n",
    "\n",
    "# Explicitly create a Graph object\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.name_scope(\"weights_and_bias\"):\n",
    "        # this time weights form a matrix, not a column vector, one \"weight\n",
    "        # vector\" per class.\n",
    "        W = tf.Variable(tf.zeros([4, 3]), name=\"weights\")\n",
    "        # so do the biases, one per class.\n",
    "        b = tf.Variable(tf.zeros([3], name=\"bias\"))\n",
    "\n",
    "    X, Y = inputs()\n",
    "    total_loss = loss(X, Y)\n",
    "    train_op = train(total_loss)\n",
    "\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        # Creates summary for output node\n",
    "        # Scalar summary for loss\n",
    "        tf.summary.scalar(\"loss\", total_loss)\n",
    "\n",
    "    # Global Variables and Operations\n",
    "    with tf.name_scope(\"global_ops\"):\n",
    "        # Initialization Op\n",
    "        init = tf.global_variables_initializer()\n",
    "        # Collect all summary Ops in graph\n",
    "        merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "# Launch the graph in a session, setup boilerplate\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Open a SummaryWriter to save summaries\n",
    "    writer = tf.summary.FileWriter(LOG_FILE, graph)\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    # actual training loop\n",
    "    training_steps = 1000\n",
    "    for step in range(training_steps):\n",
    "        sess.run([train_op])\n",
    "        # for debugging and learning purposes, see how the loss gets\n",
    "        # decremented thru training steps\n",
    "        if step % 10 == 0:\n",
    "            loss_val, summary_str = sess.run([total_loss, merged_summaries])\n",
    "            writer.add_summary(summary_str, step)\n",
    "            if step % 100 == 0:\n",
    "                print(\"loss: \", loss_val)\n",
    "\n",
    "    evaluate(sess, X, Y)\n",
    "\n",
    "    # Writes the summaries to disk\n",
    "    writer.flush()\n",
    "\n",
    "    # Flushes the summaries to disk and closes the SummaryWriter\n",
    "    writer.close()\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard Graph\n",
    "![](img/p3_tb_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard Scalar summary for loss function\n",
    "![](img/p3_tb_summaries.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "loss:  0.371925<br>\n",
    "Evaluation:  0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4. \n",
    "Analyze all relevant and non-obvious individual steps in the script, softwmax_irises.py by examining their inputs and outputs. When convenient, use existing Iris Dataset. When convenient, you are welcome to provide your own inputs. Please examine and describe actions of functions and operations within those functions: \n",
    "combine_inputs(), line 13\n",
    "inference(), line 17\n",
    "read_csv(), line 25\n",
    "decode_csv() line 34\n",
    "train.shuffle_batch(), line 37\n",
    "inputs(), line 43\n",
    "label_number = tf.to_int32(…), line 49\n",
    "features = tf.transpose(..), line 57\n",
    "evaluate(), line 67\n",
    "predicted = tf.cast(tf.arg_max(inference(X), 1).., line 69\n",
    "tf.reduce_mean(tf.cast(tf.equal(predicted,Y),.,line 71\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord).., line 85\n",
    "\n",
    "Please describe the effect of every function or command by providing an illustrative input and output set of values and well as a brief narrative. Please rely on TensorFlow API as much as possible. (%25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### combine_inputs() method\n",
    "This method combine all features and makes a (?, 3) shape matrix. Note 3 is the number of labels and ? is the batch size, 100 by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def combine_inputs(X):\n",
    "    with tf.name_scope(\"combine_inputs\"):\n",
    "        return_val = tf.matmul(X, W) + b\n",
    "        if DEBUG:\n",
    "            return_val = tf.Print(return_val, \n",
    "            [return_val, tf.shape(return_val)], \"combine_inputs = \", summarize=10)\n",
    "        return return_val\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample inputs (with two records):**<br>\n",
    "b shape [3]= [0.0066666659 -0.0033333334 -0.0033333334]<br>\n",
    "W shape [4 3]= [[0.031333331 -0.015666667 -0.015666667][0.020999998 -0.010500001 -0.010500001][0.009\n",
    "6666655 -0.0048333332 -0.0048333332][0.0013333332]...]<br>\n",
    "X shape [2 4]= [[5.1 3.8 1.5 0.3][5.4 3.9 1.7 0.4]]<br>\n",
    "\n",
    "**Sample output:**<br>\n",
    "combine_inputs shape [2 3]= [[0.26116663 -0.13058333 -0.13058333][0.2747333 -0.13736668 -0.13736668]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### inference() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method computes softmax activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Input (with two records) **<br>\n",
    "X shape [2 4] = [[5.1 3.3 1.7 0.5][5.1 3.7 1.5 0.4]] <br>\n",
    "**Sample output:**<br>\n",
    "inference shape [2 3] = [[0.425876 0.28706202 0.28706202][0.42834732 0.28582633 0.28582633]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### read_csv() method\n",
    "Reads the input csv file in given batch size (100). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### decode_csv() line 34 \n",
    "Convert CSV records to tensors. Each column maps to one tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### train.shuffle_batch(), line 37 \n",
    "Creates batches by randomly shuffling tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inputs(), line 43. Code with debug prints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def inputs():\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        sepal_length, sepal_width, petal_length, petal_width, label =\\\n",
    "            read_csv(100, DATA_FILE, [[0.0], [0.0], [0.0], [0.0], [\"\"]])\n",
    "\n",
    "        # convert class names to a 0 based class index.\n",
    "        label_number = tf.to_int32(tf.argmax(tf.to_int32(tf.stack([\n",
    "            tf.equal(label, [\"Iris-setosa\"]),\n",
    "            tf.equal(label, [\"Iris-versicolor\"]),\n",
    "            tf.equal(label, [\"Iris-virginica\"])\n",
    "        ])), 0), name=\"label\")\n",
    "\n",
    "        # Pack all the features that we care about in a single matrix;\n",
    "        # We then transpose to have a matrix with one example per row and one\n",
    "        # feature per column.\n",
    "        features = tf.transpose(tf.stack(\n",
    "            [sepal_length, sepal_width, petal_length, petal_width]), name=\"features\")\n",
    "\n",
    "        if DEBUG:\n",
    "            sepal_length = tf.Print(sepal_length, [sepal_length], \"sepal_length = \")\n",
    "            sepal_width = tf.Print(sepal_width, [sepal_width], \"sepal_width = \")\n",
    "            petal_length = tf.Print(petal_length, [petal_length], \"petal_length = \")\n",
    "            petal_width = tf.Print(petal_width, [petal_width], \"petal_width = \")\n",
    "            label_number = tf.Print(label_number, [label, label_number], \"label_number = \")\n",
    "            features = tf.Print(features, [features], \"features = \", summarize=5)\n",
    "        return features, label_number\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sepal_length = [7.1 5.8 6.3...]<br>\n",
    "sepal_width = [3 2.8 2.9...]<br>\n",
    "petal_width = [2.1 2.4 1.8...]<br>\n",
    "petal_length = [5.9 5.1 5.6...]<br>\n",
    "label_number = [Iris-virginica Iris-virginica Iris-virginica...][2 2 2...]<br>\n",
    "features = [[7.1 3 5.9 2.1][5.8]...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input() method reads the csv file 100 record batches. Then it converts labels to 0 based index. <br>\n",
    "* * *\n",
    "For example:\n",
    "label = [Iris-virginica Iris-virginica Iris-versicolor...] will be converted to label_number = [2 2 1...]. This creates a vector of shape [100]\n",
    "* * *\n",
    "Then it creates a matrix of features stacks them like. This results in a matrix of [100 4]:\n",
    "features = [[7.1 3 5.9 2.1][5.8...]...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label_number = tf.to_int32(…), line 49\n",
    "Convert labels into 0 based index:\n",
    "\n",
    "label = [Iris-virginica Iris-virginica Iris-versicolor...] will be converted to label_number = [2 2 1...]. This creates a vector of shape [100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features = tf.transpose(..), line 57 \n",
    "Creates a matrix of features stacks them like. This results in a matrix of [100 4]:\n",
    "features = [[7.1 3 5.9 2.1][5.8...]...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate(), line 67 \n",
    "Calculates the predicted label. Then measures the accuracy compared to actual value.<br>\n",
    "**Sample output (with two records): **<br>\n",
    "b shape[3]= [0.0066666659 -0.0033333334 -0.0033333334]<br>\n",
    "W shape[4 3] = [[0.034333333 -0.017166667 -0.017166667][0.022999998 -0.011500001 -0.011500001][0.010\n",
    "333333 -0.005166667 -0.005166667][0.0019999999]...]<br>\n",
    "X shape[2 4] = [[4.8 3 1.4 0.1][4.6 3.6 1 0.2]]<br>\n",
    "inference = [[0.42300561 0.28849721 0.28849721][0.42410427 0.28794786 0.28794786]][2 3]<br>\n",
    "**Sample output: **<br>\n",
    "predicted shape[2] = [0 0]<br>\n",
    "Evaluation:  1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predicted = tf.cast(tf.arg_max(inference(X), 1).., line 69 \n",
    "Calculate the predicte value in 0 based index. <br>\n",
    "ex:<br>\n",
    "predicted shape[2] = [0 0]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.reduce_mean(tf.cast(tf.equal(predicted,Y),.,line 71 \n",
    "Compare actual to predicted and calculate how accurate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threads = tf.train.start_queue_runners(sess=sess, coord=coord).., line 85\n",
    "Starts all queue runners collected in the graph.<br>\n",
    "\n",
    "This is a companion method to add_queue_runner(). It just starts threads for all queue runners collected in the graph. It returns the list of all threads."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
